<!DOCTYPE html> <html lang="pl"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content=""> <meta name="msvalidate.01" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Jak uruchomić LLM lokalnie | Szymon Kowalewski</title> <meta name="author" content="Szymon Kowalewski"> <meta name="description" content="Krótki artykuł o tym jak uruchomić LLM lokalnie."> <meta name="keywords" content="portfolio-website, blog, news"> <meta property="og:site_name" content="Szymon Kowalewski"> <meta property="og:type" content="article"> <meta property="og:title" content="Szymon Kowalewski | Jak uruchomić LLM lokalnie"> <meta property="og:url" content="https://Szymok.github.io/blog/2023/llm-local/"> <meta property="og:description" content="Krótki artykuł o tym jak uruchomić LLM lokalnie."> <meta property="og:locale" content="pl"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Jak uruchomić LLM lokalnie"> <meta name="twitter:description" content="Krótki artykuł o tym jak uruchomić LLM lokalnie."> <meta name="twitter:site" content="@SkSzymon"> <meta name="twitter:creator" content="@SkSzymon"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://szymok.github.io/blog/2023/llm-local/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Szymon </span>Kowalewski</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/newsletter/">newsletter</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="http://quartz.skszymon.eu" rel="external nofollow noopener" target="_blank">ogród wiedzy</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/cv/">cv</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Jak uruchomić LLM lokalnie</h1> <p class="post-meta">August 1, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/ai"> <i class="fas fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/tag/sztuczna-inteligencja"> <i class="fas fa-hashtag fa-sm"></i> sztuczna-inteligencja</a>   <a href="/blog/tag/generatywne-ai"> <i class="fas fa-hashtag fa-sm"></i> generatywne-ai</a>   <a href="/blog/tag/llm"> <i class="fas fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/llama"> <i class="fas fa-hashtag fa-sm"></i> llama</a>   <a href="/blog/tag/alpaca"> <i class="fas fa-hashtag fa-sm"></i> alpaca</a>     ·   <a href="/blog/category/article"> <i class="fas fa-tag fa-sm"></i> article</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/infidel0986_a_generative_AI_LLM_hot_air_balloon_with_business_p_82aac953-8aec-429d-b858-1c84f034be75-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/infidel0986_a_generative_AI_LLM_hot_air_balloon_with_business_p_82aac953-8aec-429d-b858-1c84f034be75-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/infidel0986_a_generative_AI_LLM_hot_air_balloon_with_business_p_82aac953-8aec-429d-b858-1c84f034be75-1400.webp"></source> <img src="/assets/img/infidel0986_a_generative_AI_LLM_hot_air_balloon_with_business_p_82aac953-8aec-429d-b858-1c84f034be75.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Obraz wygenerowany przy pomocy Midjourney oraz edytowany w Photoshopie przy pomoocy funkcji Generate AI. </div> <h2 id="uruchamianie-modeli-llm-lokalnie-za-pomocą-biblioteki-dalai">Uruchamianie modeli LLM lokalnie za pomocą biblioteki Dalai</h2> <p>Czy kiedykolwiek zastanawiałeś się nad uruchomieniem zaawansowanych modeli językowych, takich jak ChatGPT, na swoim własnym komputerze? Nie jesteś w tej przygodzie sam! W tym przewodniku krok po kroku dowiemy się, jak to zrobić za pomocą modeli LLaMA i Alpaca, korzystając z biblioteki Dalai. Bądź gotowy na szczegółową, krok po kroku i interesującą podróż do świata AI!</p> <h3 id="czym-jest-biblioteka-dalai">Czym jest biblioteka Dalai?</h3> <p>Dalai to narzędzie opracowane przez Meta AI w celu umożliwienia użytkownikom korzystania z modeli językowych LLM na swoich własnych komputerach. Dzięki tej bibliotece, możemy wykorzystać modele LLM, takie jak LLaMA i Alpaca, w środowisku offline, co eliminuje konieczność korzystania z centralnych i często komercyjnych rozwiązań oferowanych przez duże firmy.</p> <h3 id="dostępność-jako-biblioteka-nodejs">Dostępność jako biblioteka Node.js</h3> <p>Dalai został stworzony jako biblioteka dla środowiska Node.js, co umożliwia łatwe i elastyczne wykorzystanie modeli LLM w aplikacjach opartych na tym środowisku. Jest to szczególnie korzystne dla programistów pracujących w języku JavaScript.</p> <h2 id="dylemat-lokalnej-sztucznej-inteligencji-nasza-własna-prywatna-ostoja">Dylemat lokalnej sztucznej inteligencji: Nasza własna prywatna ostoja</h2> <p>Mierzymy się z wieloma możliwościami, jakie model generatywnej sztucznej inteligencji nam przynosi. Jednak wraz z wielką mocą przychodzi wielka odpowiedzialność (dzięki, Wujek Ben). Jednym z największych obaw, które mamy, jest prywatność danych. Centralizowane modele oferowane przez OpenAI i Microsoft są fantastyczne, ale czy naprawdę chcemy oddać nasze dane na srebrnej tacy?</p> <p>Wyobraź sobie, że Batman musiałby podzielić się lokalizacją Batjaskini ze wszystkimi. Nie byłoby to zbyt fajne, prawda? Tutaj wchodzi w grę uruchomienie modelu AI na swoim lokalnym komputerze. To jak posiadanie swojej własnej Batjaskini (bez fajnych gadżetów i pojazdów w stylu nietoperza, oczywiście).</p> <p>Plusy uruchamiania LLM na maszynach lokalnych:</p> <ul> <li>Prywatność danych: Jest to jedna z największych zalet uruchamiania modeli LLM na maszynach lokalnych. Korzystając z własnej infrastruktury, użytkownik ma większą kontrolę nad swoimi danymi i unika przekazywania ich do zewnętrznych serwerów lub chmur. Dla osób lub organizacji, które szczególnie dbają o prywatność danych, jest to kluczowe.</li> <li>Szybkość działania: Uruchamianie modeli LLM na lokalnych maszynach może być szybsze niż korzystanie z modeli działających w chmurze. Dzięki temu można uzyskać wyniki generowania tekstu błyskawicznie, bez opóźnień związanych z przesyłaniem danych do zdalnych serwerów.</li> <li>Brak opłat za korzystanie: Często korzystanie z modeli LLM w chmurze może być powiązane z opłatami, które rosną w miarę zwiększania ilości generowanego tekstu. Uruchamianie modeli lokalnie może pozwolić uniknąć tych kosztów i oszczędzić na długoterminowej współpracy.</li> <li>Modyfikowalność i dostosowywanie: Korzystając z maszyn lokalnych, użytkownik ma pełną kontrolę nad konfiguracją i dostosowaniem modeli LLM. Można zmieniać parametry, testować różne warianty modeli i dostosowywać je do swoich potrzeb.</li> </ul> <p>Minusy uruchamiania LLM na maszynach lokalnych:</p> <ul> <li>Wymagania sprzętowe: Niektóre modele LLM, zwłaszcza te zaawansowane, mogą wymagać znacznych zasobów sprzętowych, takich jak duża ilość pamięci RAM czy mocny procesor. Uruchomienie ich na komputerze osobistym może być utrudnione lub niemożliwe ze względu na ograniczenia sprzętowe.</li> <li>Kompleksowość instalacji: Instalacja i konfiguracja modeli LLM na lokalnych maszynach może być skomplikowana, szczególnie dla osób bez doświadczenia w programowaniu czy obszarze sztucznej inteligencji. Wymaga to znalezienia odpowiednich wersji bibliotek, narzędzi i zależności, co może być czasochłonne i frustrujące.</li> <li>Brak skalowalności: Uruchomienie modelu LLM na maszynie lokalnej ogranicza skalowalność generowania tekstu. Jeśli potrzebujemy dużej ilości generowanego tekstu lub jednoczesnego dostępu wielu użytkowników, lokalne środowisko może nie być wystarczające.</li> <li>Ograniczona aktualizacja modeli: W porównaniu do korzystania z modeli LLM w chmurze, aktualizacje modeli mogą być trudniejsze do przeprowadzenia na maszynach lokalnych. Wymaga to ręcznej aktualizacji i utrzymania modelu, co może być problematyczne, gdy pojawią się nowe i ulepszone wersje modeli.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Screenshot%202023-08-01%20232108-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Screenshot%202023-08-01%20232108-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Screenshot%202023-08-01%20232108-1400.webp"></source> <img src="/assets/img/Screenshot%202023-08-01%20232108.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Diagram z artykułu opublikowanego w magazynie Research on Foundation Models (CRFM) w Stanford. Źródło: <a href="https://crfm.stanford.edu/" rel="external nofollow noopener" target="_blank">CRFM</a> </div> <h2 id="llama-kompaktowy-potwór-stworzony-przez-meta-ai">LLaMA: Kompaktowy potwór stworzony przez Meta AI</h2> <p>LLaMA to podstawowy model językowy, który osiągnął coś niesamowitego. Mimo że jest 13 razy mniejszy od kolosalnego GPT-3, przewyższa ten ostatni na większości benchmarków! Ten kompaktowy potwór może być uruchamiany na lokalnych maszynach - jedna odważna jednostka nawet zdołała go uruchomić na Raspberry Pi!</p> <p>Teraz, dzięki pewnym nieprzewidzianym okolicznościom, LLaMA jest dostępny do użytku niekomercyjnego. Został stworzony przez utalentowany zespół w Meta AI, a LLaMA oraz jego “rodzeństwo” - Alpaca - sprawiają, że lokalne wykorzystanie sztucznej inteligencji staje się bardziej dostępne niż kiedykolwiek wcześniej. Więc, bez zbędnych zwłok, zaczynajmy tę wspaniałą podróż!</p> <p>Pierwszym z brzegu pomysłem na skorzystaniu z wymiarów Raspberry PI byłoby stworzenie stworzenie salonu, gdzie każde urządzenie polegałoby na osobnym modelu i użytkownicy mogliby się z nimi komunikować, jednocześnie tworząc wirtualną historie. Byłoby to o tyle ciekawe, że spędzenie pewnego czasu w tym lokalu, posunięcie historii dalej i pozostawienie jej w tym samym miejscu dla kolejnych klientów lokalu. W ten sposób każdy mógłby wnieść swój wkład w historię, a jednocześnie cieszyć się z tego, co stworzyli inni.</p> <h2 id="instalacja-i-uruchamianie-llama">Instalacja i uruchamianie LLaMA</h2> <p>Sklonuj repozytorium i zainstaluj niezbędne wymagania z https://gichub.com/cockroiJpeanuVdolai.</p> <p>Aby rozpocząć, uruchom polecenie:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">npx</span> <span class="n">dalaî</span> <span class="n">install</span> <span class="mi">7</span><span class="n">B</span>
</code></pre></div></div> <p>Zanim przejdziesz dalej, upewnij się, że LLaMA-7B potrzebuje około 31 GB pamięci. Sprawdź, czy na twoim komputerze jest wystarczająco dużo miejsca na tego małego, ale potężnego gościa. Sam miałem z tym sporo problemów!</p> <p>Aby uruchomić LLaMA, wystarczy wpisać:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">npx</span> <span class="n">dalai</span> <span class="n">serves</span>
</code></pre></div></div> <p>I to wszystko! Masz teraz duży model językowy działający lokalnie. Gratulacje! Dlaczego? Ponieważ musiałem przejść przez wiele trudności, aby to uruchomić, takie jak dopasowywanie określonych wersji Pythona i Node. Szczegóły możesz znaleźć w pliku Readme na stronie https://github.com/cocktailpeaonut/dalai.</p> <p>Jednak kiedy to uruchomiłem, otrzymałem mnóstwo bezsensownych ciągów liter w odpowiedzi na proste pytanie “Mam ochotę coś zjeść, ponieważ…”. Po dokładniejszym zbadaniu wygląda na to, że jest to znany problem. Ktoś w kanale dyskusji sugerował: “Sprawdź model Alpaca”, więc to zrobiłem.</p> <h2 id="alpaca-cudo-które-podąża-za-instrukcjami">Alpaca: Cudo, które podąża za instrukcjami</h2> <p>Alpaca to wersja fine-tuningowana LLaMA, zaprojektowana tak, aby podążała za instrukcjami, podobnie jak ChatGPT. Niesamowite jest to, że cały proces dopasowania kosztował mniej niż 600 dolarów! Porównując to z ogromną ceną 5 milionów dolarów za GPT-3.5, to prawdziwa okazja!</p> <p>Jak to osiągnięto? Model tekstu OpenAI - davinci-003 - nieświadomie pomógł, przekształcając 175 zadań samouczka w aż 52 000 przykładów podążających za instrukcjami do nadzorowanego dopasowania. Mówię o inteligentnym rozwiązaniu! Autorami tego niesamowitego modelu są Rohan Taori i inni. To takie kreatywne - praktycznie użyli da-vinci-03 jako nauczyciela dla LLaMA, aby stworzyć Alpacę! Całą pracę można znaleźć w artykule na stronie htps://crfm.stanford.edu/2023/03/13/alpaco.html</p> <h2 id="instalacja-i-uruchamianie-alpaca">Instalacja i uruchamianie Alpaca</h2> <p>Aby zainstalować Alpacę, wystarczy uruchomić:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">npx</span> <span class="n">dalai</span> <span class="n">alpaca</span> <span class="n">install</span> <span class="mi">78</span>
</code></pre></div></div> <p>Alpaca to lekki model, który wymaga tylko 4 GB pamięci, więc nie zajmie wiele miejsca na twoim komputerze. Aby uruchomić Alpacę, po prostu powtórz polecenie:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">npx</span> <span class="n">dalai</span> <span class="n">alpaca</span> <span class="n">serves</span>
</code></pre></div></div> <p>I gotowe! Masz teraz własny model ChatGPT, gotowy do działania!</p> <h2 id="wszechstronne-api-dalai">Wszechstronne API Dalai</h2> <p>To nie koniec zabawy - biblioteka Dalai oferuje również interfejs API, który umożliwia integrację zarówno LLaMA, jak i Alpaki w twoje własne aplikacje. Otwiera to świat możliwości dla innowacyjnych projektów i eksperymentów na twoim lokalnym komputerze.</p> <p>Pomyśl o stworzeniu swojego własnego chatbota opartego na sztucznej inteligencji, budowaniu inteligentnego asystenta do pisania czy nawet opracowaniu nauczyciela AI dla twojego ulubionego przedmiotu! Teraz, kiedy nie jesteś ograniczony do 32 tysięcy słów, możesz karmić modele wszystkimi klasykami napisanymi przez swojego ulubionego autora (który już nie żyje, aby dać nam więcej magicznych dzieł).</p> <p>Baw się dobrze z lokalnymi modelami LLM i ciesz się wspaniałymi możliwościami, jakie oferują. Sztuczna inteligencja staje się coraz bardziej dostępna, co pozwala nam na bardziej kreatywną i interesującą pracę z modelami językowymi na naszych własnych komputerach.</p> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"Szymok/Szymok.github.io","data-repo-id":"R_kgDOI7kDZQ","data-category":"General","data-category-id":"DIC_kwDOI7kDZc4CUF8S","data-mapping":"title","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"pl",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Szymon Kowalewski. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/Szymok/Szymok.github.io" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: November 30, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-GY820WSFQY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-GY820WSFQY");</script> <script>!function(e,t,a,n,r){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var g=t.getElementsByTagName(a)[0],m=t.createElement(a),s="dataLayer"!=n?"&l="+n:"";m.async=!0,m.src="https://www.googletagmanager.com/gtm.js?id="+r+s,g.parentNode.insertBefore(m,g)}(window,document,"script","dataLayer","GTM-M729QXX");</script> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-M729QXX" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.css"> <script src="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.js" data-cfasync="false"></script> <script>function getCookie(o){var t=document.cookie.match("(^|[^;]+)\\s*"+o+"\\s*=\\s*([^;]+)");return t?t.pop():""}function addAnalytics(){function o(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],o("js",new Date),o("config","G-GY820WSFQY")}window.addEventListener("load",function(){const o=getCookie("cookieconsent_status");"allow"!==o&&""!==o||addAnalytics(),window.cookieconsent.initialise({palette:{popup:{background:"#efefef",text:"#404040"},button:{background:"#8ec760",text:"#ffffff"}},type:"opt-out",content:{allow:"Approve",dismiss:"Approve",deny:"Reject"},onStatusChange:function(){location.reload()}})});</script> </body> </html>